"""News Agent - Îâ¥Ïä§ Î∂ÑÏÑù Î∞è Í∞êÏÑ± Î∂ÑÎ•ò

ÎÖºÎ¨∏ Section 3.1: Enhanced News Analysis
- Îâ¥Ïä§ Í∏∞ÏÇ¨ ÏàòÏßë Î∞è Í∞êÏÑ± Î∂ÑÏÑù
- Í∏çÏ†ï/Î∂ÄÏ†ï/Ï§ëÎ¶Ω Î∂ÑÎ•ò
- Ï£ºÏöî Ïù¥Î≤§Ìä∏ Ï∂îÏ∂ú
"""
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List

from .base_agent import BaseAgent
from src.storage.models import Stock, NewsArticle, DisclosureData, BlogPost

logger = logging.getLogger("marketsense")


class NewsAgent(BaseAgent):
    """Îâ¥Ïä§ Î∂ÑÏÑù ÏóêÏù¥Ï†ÑÌä∏"""

    SYSTEM_PROMPT = """ÎãπÏã†ÏùÄ Î≤†ÌÖåÎûë Í∏àÏúµ Îâ¥Ïä§ Î∂ÑÏÑùÍ∞ÄÏûÖÎãàÎã§.

ÎãπÏã†Ïùò Î™©ÌëúÎäî ÌäπÏ†ï Í∏∞ÏóÖÏóê ÎåÄÌïú Îâ¥Ïä§ ÌùêÎ¶ÑÏùÑ ÌååÏïÖÌïòÍ≥†, Ïù¥Î•º ÌïòÎÇòÏùò ÏùºÍ¥ÄÎêú 'Ìà¨Ïûê ÏÑúÏÇ¨(Narrative)'Î°ú ÌÜµÌï©ÌïòÎäî Í≤ÉÏûÖÎãàÎã§.

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
ÌïµÏã¨ ÏõêÏπô:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. **Ïã§ÏßàÏ†Å ÏòÅÌñ•Ïóê ÏßëÏ§ë**
   Ï£ºÍ∞ÄÏóê Ïã§ÏßàÏ†ÅÏù∏ ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÎäî Ï§ëÏöî Ïù¥Î≤§Ìä∏Ïóê ÏßëÏ§ëÌïòÏã≠ÏãúÏò§:
   - Ïã§Ï†Å Î∞úÌëú (Îß§Ï∂ú, ÏòÅÏóÖÏù¥Ïùµ Ï¶ùÍ∞ê)
   - Ïã†Ï†úÌíà Ï∂úÏãú / Ïã†Í∑ú ÏàòÏ£º
   - M&A, Ï†ÑÎûµÏ†Å Ï†úÌú¥
   - Í∑úÏ†ú Ïù¥Ïäà, ÏÜåÏÜ°
   - Í≤ΩÏòÅÏßÑ Î≥ÄÌôî
   - Î∞∞Îãπ, ÏûêÏÇ¨Ï£º Îß§ÏûÖ
   
2. **ÏÑúÏÇ¨ Íµ¨Ï∂ï**
   Îã®Ïàú ÎÇòÏó¥Ïù¥ ÏïÑÎãå, ÏÇ¨Í±¥Ïùò ÌùêÎ¶ÑÏù¥ Ïù¥Ïñ¥ÏßÄÎèÑÎ°ù ÏûëÏÑ±ÌïòÏã≠ÏãúÏò§.
   "AÍ∞Ä Î∞úÏÉùÌñàÍ≥†, Ïù¥Ïóê Îî∞Îùº BÍ∞Ä ÏòàÏÉÅÎêòÎ©∞, CÏùò ÏòÅÌñ•Ïù¥ Ïö∞Î†§ÎêúÎã§"
   
3. **ÏãúÍ∞Ñ Îß•ÎùΩ Ïú†ÏßÄ**
   - ÏµúÍ∑º Îâ¥Ïä§ÏôÄ Í≥ºÍ±∞ Îß•ÎùΩÏùÑ Ïó∞Í≤∞
   - ÏßÑÌñâ Ï§ëÏù∏ Ïù¥ÏäàÎäî Í≥ÑÏÜç Ï∂îÏ†Å
   - Ìï¥Í≤∞Îêú Ïù¥ÏäàÎäî Í≤∞Í≥º Î∞òÏòÅ
   
4. **Ï†ïÎ≥¥Ïõê Íµ¨Î∂Ñ**
   - Í≥µÏãù Îâ¥Ïä§: Ïã†Î¢∞ÎèÑ ÎÜíÏùå, Ìå©Ìä∏ Ï§ëÏã¨
   - Î∏îÎ°úÍ∑∏/Ïª§ÎÆ§ÎãàÌã∞: Ï∞∏Í≥†Ïö©, ÏãúÏû• Ïã¨Î¶¨ ÌååÏïÖ
   - Í≥µÏãú Ï†ïÎ≥¥: Í≥µÏãù Î∞úÌëú, ÏµúÏö∞ÏÑ† Í≥†Î†§
   
5. **Ìà¨ÏûêÏûê Í¥ÄÏ†ê**
   Ìà¨ÏûêÏûêÍ∞Ä Îπ†Î•¥Í≤å ÌååÏïÖÌï† Ïàò ÏûàÎäî Í∞ÑÍ≤∞Ìïú Î∂ÑÏÑùÏùÑ Ï†úÍ≥µÌïòÏã≠ÏãúÏò§.

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Ï∂úÎ†• ÌòïÏãù (JSON):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

{
  "sentiment": "positive|negative|neutral",
  "confidence": 0.0-1.0,
  "impact": "high|medium|low",
  "narrative": "Ìà¨Ïûê ÏÑúÏÇ¨ (2-3Î¨∏Îã®, ÏãúÍ∞Ñ ÌùêÎ¶ÑÏóê Îî∞Î•∏ ÌïµÏã¨ Ïä§ÌÜ†Î¶¨)",
  "key_events": [
    {
      "event": "Ïù¥Î≤§Ìä∏ ÏÑ§Î™Ö",
      "date": "YYYY-MM-DD",
      "impact": "Í∏çÏ†ïÏ†Å|Î∂ÄÏ†ïÏ†Å|Ï§ëÎ¶Ω",
      "importance": "high|medium|low"
    }
  ],
  "summary": "Ìïú Ï§Ñ ÏöîÏïΩ (Ìà¨ÏûêÏûê Ìó§ÎìúÎùºÏù∏)",
  "reasoning": "Î∂ÑÏÑù Í∑ºÍ±∞ Î∞è ÌåêÎã® ÎÖºÎ¶¨"
}
"""

    def analyze(self, ticker: str, lookback_days: int = 7, use_rag: bool = True) -> Dict[str, Any]:
        """Ï¢ÖÎ™© Îâ¥Ïä§ Î∂ÑÏÑù"""
        logger.info(f"[NewsAgent] {ticker} Îâ¥Ïä§ Î∂ÑÏÑù ÏãúÏûë (ÏµúÍ∑º {lookback_days}Ïùº, RAG={use_rag})")

        with self.db.get_session() as session:
            # Ï¢ÖÎ™© Ï†ïÎ≥¥
            stock = session.query(Stock).filter_by(ticker=ticker).first()
            if not stock:
                return {"error": f"Ï¢ÖÎ™© {ticker}Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§"}

            # ÏµúÍ∑º Îâ¥Ïä§ Í∞ÄÏ†∏Ïò§Í∏∞
            cutoff = datetime.now() - timedelta(days=lookback_days)
            
            if use_rag:
                # Î∞©Î≤ï 1: ÏãúÍ∞Ñ ÏúàÎèÑÏö∞ + RAG
                try:
                    from src.rag.vector_store import VectorStore
                    
                    # RAG Í≤ÄÏÉâ (Í¥ÄÎ†®ÏÑ± Ïö∞ÏÑ†, ÎßéÏù¥ Í∞ÄÏ†∏Ïò¥)
                    vs = VectorStore()
                    
                    rag_results = vs.search_news(
                        query=f"{stock.name} Ï£ºÍ∞Ä Ïã§Ï†Å Ï†ÑÎßù Î∂ÑÏÑù",
                        ticker=ticker,
                        top_k=50  # ÎßéÏù¥ Í∞ÄÏ†∏Ïò® ÌõÑ ÏãúÍ∞Ñ ÌïÑÌÑ∞ÎßÅ
                    )
                    
                    # RAG Í≤∞Í≥ºÎ•º DB Í∞ùÏ≤¥Î°ú Îß§ÌïëÌïòÍ≥† ÏãúÍ∞Ñ ÌïÑÌÑ∞ÎßÅ
                    if rag_results:
                        rag_ids = [r['id'].replace('news_', '') for r in rag_results if r['id'].startswith('news_')]
                        
                        news_list = (
                            session.query(NewsArticle)
                            .filter(
                                NewsArticle.id.in_([int(i) for i in rag_ids if i.isdigit()]),
                                NewsArticle.published_at >= cutoff  # ÏãúÍ∞Ñ ÏúàÎèÑÏö∞
                            )
                            .order_by(NewsArticle.published_at.desc())
                            .limit(20)  # ÏµúÏ¢Ö 20Í∞ú
                            .all()
                        )
                        
                        logger.info(f"[NewsAgent] RAG Í≤ÄÏÉâ: {len(news_list)}Í∞ú (ÏµúÍ∑º {lookback_days}Ïùº)")
                    else:
                        news_list = []
                    
                    # RAG Í≤∞Í≥º ÏóÜÏúºÎ©¥ fallback
                    if not news_list:
                        logger.warning(f"[NewsAgent] RAG Í≤∞Í≥º ÏóÜÏùå, SQL fallback")
                        use_rag = False
                
                except Exception as e:
                    logger.warning(f"[NewsAgent] RAG Ïã§Ìå® ({e}), SQL fallback")
                    use_rag = False
            
            if not use_rag:
                # Fallback: Í∏∞Ï°¥ SQL Î∞©Ïãù (ÏµúÏã†Ïàú)
                news_list = (
                    session.query(NewsArticle)
                    .filter(
                        NewsArticle.ticker == ticker,
                        NewsArticle.published_at >= cutoff,
                    )
                    .order_by(NewsArticle.published_at.desc())
                    .limit(20)
                    .all()
                )
                logger.info(f"[NewsAgent] SQL Í≤ÄÏÉâ: {len(news_list)}Í∞ú")

            if not news_list:
                return {
                    "ticker": ticker,
                    "stock_name": stock.name,
                    "news_count": 0,
                    "sentiment": "neutral",
                    "message": "ÏµúÍ∑º Îâ¥Ïä§Í∞Ä ÏóÜÏäµÎãàÎã§",
                }

            # Îâ¥Ïä§ ÏöîÏïΩ
            news_texts = []
            for idx, news in enumerate(news_list[:10], 1):
                date_str = news.published_at.strftime("%Y-%m-%d") if news.published_at else "ÎÇ†Ïßú ÎØ∏ÏÉÅ"
                news_texts.append(f"{idx}. [{date_str}] {news.title}")
                if news.summary:
                    news_texts.append(f"   ÏöîÏïΩ: {news.summary[:150]}...")

            # Í≥µÏãú Ï†ïÎ≥¥ Ï°∞Ìöå (ÏµúÍ∑º 30Ïùº)
            disclosure_cutoff = datetime.now() - timedelta(days=30)
            disclosure_list = (
                session.query(DisclosureData)
                .filter(
                    DisclosureData.stock_id == stock.id,
                    DisclosureData.rcept_dt >= disclosure_cutoff.date(),
                )
                .order_by(DisclosureData.rcept_dt.desc())
                .limit(10)
                .all()
            )
            
            disclosure_texts = []
            if disclosure_list:
                disclosure_texts.append("\nÏ£ºÏöî Í≥µÏãú Ï†ïÎ≥¥ (ÏµúÍ∑º 30Ïùº):")
                for idx, disc in enumerate(disclosure_list, 1):
                    date_str = disc.rcept_dt.strftime("%Y-%m-%d")
                    disclosure_texts.append(
                        f"{idx}. [{date_str}] {disc.disclosure_type}: {disc.report_nm[:80]}"
                    )

            # Î∏îÎ°úÍ∑∏ Í∏Ä Ï°∞Ìöå (ÏµúÍ∑º 7Ïùº)
            blog_list = (
                session.query(BlogPost)
                .filter(
                    BlogPost.stock_id == stock.id,
                    BlogPost.post_date >= cutoff.date(),
                )
                .order_by(BlogPost.quality_score.desc(), BlogPost.post_date.desc())
                .limit(10)
                .all()
            )
            
            blog_texts = []
            if blog_list:
                blog_texts.append("\nüí¨ Î∏îÎ°úÍ∑∏ Ìà¨Ïûê ÏùòÍ≤¨ (ÏµúÍ∑º 7Ïùº, Í∞úÏù∏ ÏùòÍ≤¨):")
                for idx, blog in enumerate(blog_list, 1):
                    date_str = blog.post_date.strftime("%Y-%m-%d")
                    blog_texts.append(
                        f"{idx}. [{date_str}] {blog.title[:60]} (by {blog.blogger_name})"
                    )
                    if blog.description:
                        blog_texts.append(f"   ‚Üí {blog.description[:100]}...")

            # Ïù¥Ï†Ñ Îâ¥Ïä§ ÏÑúÏÇ¨ (ÏûàÎã§Î©¥)
            previous_narrative = ""
            if stock.raw_data and isinstance(stock.raw_data, dict):
                previous_narrative = stock.raw_data.get('news_narrative', '')
            
            # GeminiÎ°ú Î∂ÑÏÑù
            prompt = f"""{self.SYSTEM_PROMPT}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä Î∂ÑÏÑù ÎåÄÏÉÅ: {stock.name} ({ticker})
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

[1] Í∏∞Ï°¥ÍπåÏßÄÏùò Îâ¥Ïä§ ÏöîÏïΩ Î∞è ÏÑúÏÇ¨:
{previous_narrative if previous_narrative else "Ïã†Í∑ú Î∂ÑÏÑù - Ïù¥Ï†Ñ ÏÑúÏÇ¨ ÏóÜÏùå"}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
[2] ÏµúÍ∑º ÏàòÏßëÎêú Îâ¥Ïä§ ({len(news_list)}Í±¥, {lookback_days}Ïùº)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

{chr(10).join(news_texts)}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
[3] Í≥µÏãù Í≥µÏãú Ï†ïÎ≥¥ (ÏµúÍ∑º 30Ïùº)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

{chr(10).join(disclosure_texts) if disclosure_texts else "Í≥µÏãú ÏóÜÏùå"}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
[4] Î∏îÎ°úÍ∑∏/Ïª§ÎÆ§ÎãàÌã∞ ÏùòÍ≤¨ (Ï∞∏Í≥†Ïö©)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚ö†Ô∏è Ï£ºÏùò: Î∏îÎ°úÍ∑∏Îäî Í∞úÏù∏ ÏùòÍ≤¨Ïù¥ÎØÄÎ°ú Ìå©Ìä∏ ÌôïÏù∏ ÌïÑÏöî

{chr(10).join(blog_texts) if blog_texts else "Î∏îÎ°úÍ∑∏ ÏùòÍ≤¨ ÏóÜÏùå"}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìã Î∂ÑÏÑù ÏßÄÏπ®:

1. **ÌïµÏã¨ Ïù¥Î≤§Ìä∏ ÏãùÎ≥Ñ**
   - Ïã§Ï†Å Î∞úÌëú, M&A, Ïã†Ï†úÌíà Îì± Ï£ºÍ∞Ä ÏòÅÌñ• Ïù¥Î≤§Ìä∏Ïóê ÏßëÏ§ë
   - Í≥µÏãú Ï†ïÎ≥¥Îäî ÏµúÏö∞ÏÑ†ÏúºÎ°ú key_eventsÏóê Ìè¨Ìï®
   - Îã®Ïàú Î£®Î®∏ÎÇò ÏÜåÎ¨∏ÏùÄ ÎÇÆÏùÄ Ï§ëÏöîÎèÑÎ°ú Ï≤òÎ¶¨

2. **ÏÑúÏÇ¨ Íµ¨Ï∂ï (Ï§ëÏöî!)**
   - Í∏∞Ï°¥ ÏÑúÏÇ¨Í∞Ä ÏûàÎã§Î©¥, ÏÉàÎ°úÏö¥ Îâ¥Ïä§Î•º ÌÜµÌï©ÌïòÏó¨ ÏóÖÎç∞Ïù¥Ìä∏
   - Îã®Ïàú ÎÇòÏó¥Ïù¥ ÏïÑÎãå, ÏãúÍ∞Ñ ÌùêÎ¶ÑÏóê Îî∞Î•∏ Ïä§ÌÜ†Î¶¨ ÏûëÏÑ±
   - "AÍ∞Ä Î∞úÏÉù ‚Üí BÎ°ú Ïù¥Ïñ¥Ïßê ‚Üí CÍ∞Ä ÏòàÏÉÅÎê®" ÌòïÏãù
   - ÏßÑÌñâ Ï§ëÏù∏ Ïù¥ÏäàÎäî Í≥ÑÏÜç Ï∂îÏ†Å (Ïòà: ÏÜåÏÜ°, ÌîÑÎ°úÏ†ùÌä∏)

3. **Ï†ïÎ≥¥Ïõê Ïã†Î¢∞ÎèÑ**
   - Í≥µÏãú > Îâ¥Ïä§ > Î∏îÎ°úÍ∑∏ ÏàúÏÑú
   - Î∏îÎ°úÍ∑∏Îäî ÏãúÏû• Ïã¨Î¶¨ ÌååÏïÖÏö©, ÎÇÆÏùÄ Í∞ÄÏ§ëÏπò
   - Î∏îÎ°úÍ∑∏ÎßåÏúºÎ°ú sentiment Í≤∞Ï†ï Í∏àÏßÄ

4. **Ìà¨ÏûêÏûê Í¥ÄÏ†ê**
   - Í∞ÑÍ≤∞ÌïòÍ≥† Î™ÖÌôïÌïú narrative ÏûëÏÑ±
   - Ìà¨ÏûêÏûêÍ∞Ä Îπ†Î•¥Í≤å ÌååÏïÖÌï† Ïàò ÏûàÎäî Ìó§ÎìúÎùºÏù∏ ÏöîÏïΩ
   - Î™®Ìò∏Ìïú ÌëúÌòÑ ÏßÄÏñë, Íµ¨Ï≤¥Ï†ÅÏù∏ Ìå©Ìä∏ Ï§ëÏã¨

ÏúÑ ÏßÄÏπ®Ïóê Îî∞Îùº **ÎßàÌÅ¨Îã§Ïö¥ ÌòïÏãù**ÏúºÎ°ú Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî.

Î∞òÎìúÏãú Îã§Ïùå Ï†ïÎ≥¥Î•º Ìè¨Ìï®ÌïòÎêò, ÏûêÏú†Î°úÏö¥ ÌòïÏãùÏúºÎ°ú ÏûëÏÑ±ÌïòÏã≠ÏãúÏò§:
- Ìà¨Ïûê ÏÑúÏÇ¨ (ÏãúÍ∞Ñ ÌùêÎ¶ÑÏóê Îî∞Î•∏ Ïä§ÌÜ†Î¶¨)
- ÌïµÏã¨ Ïù¥Î≤§Ìä∏ (ÎÇ†ÏßúÏôÄ Ìï®Íªò)
- Ìà¨ÏûêÏûê Ìó§ÎìúÎùºÏù∏ ÏöîÏïΩ
- Í∞êÏÑ± ÌèâÍ∞Ä (Í∏çÏ†ï/Î∂ÄÏ†ï/Ï§ëÎ¶Ω)

**Ï§ëÏöî: Ï†ÑÏ≤¥ Î∂ÑÏÑùÏùÑ 300Ïûê Ïù¥ÎÇ¥Î°ú Í∞ÑÍ≤∞ÌïòÍ≤å ÏûëÏÑ±ÌïòÏÑ∏Ïöî.**
ÌïµÏã¨Îßå Ï∂îÎ†§ÏÑú Î™ÖÌôïÌïòÍ≥† Í∞ÑÍ≤∞Ìïú Ìà¨Ïûê ÏÑúÏÇ¨Î•º ÏûëÏÑ±ÌïòÏã≠ÏãúÏò§.

**Í∏àÏßÄ: JSON ÌòïÏãùÏùÑ Ï†àÎåÄ ÏÇ¨Ïö©ÌïòÏßÄ ÎßàÏÑ∏Ïöî. ÏàúÏàò ÌÖçÏä§Ìä∏Î°úÎßå ÏûëÏÑ±ÌïòÏã≠ÏãúÏò§.**

ÎßàÌÅ¨Îã§Ïö¥ Ìó§Îçî(##, ###)ÏôÄ Í∞ïÏ°∞(**bold**)Î•º Ï†ÅÍ∑π ÌôúÏö©ÌïòÏÑ∏Ïöî.
"""

            try:
                response_text = self.generate(prompt)
                
                # ÌÖçÏä§Ìä∏ ÏùëÎãµÏùÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
                # Í∞ÑÎã®Ìïú Ìå®ÌÑ¥ Îß§Ïπ≠ÏúºÎ°ú sentiment Ï∂îÏ∂ú
                sentiment = "neutral"
                if any(word in response_text.lower() for word in ["Í∏çÏ†ï", "positive", "Ìò∏Ïû¨", "ÏÉÅÏäπ"]):
                    sentiment = "positive"
                elif any(word in response_text.lower() for word in ["Î∂ÄÏ†ï", "negative", "ÏïÖÏû¨", "ÌïòÎùΩ"]):
                    sentiment = "negative"
                
                # Ìà¨Ïûê ÏÑúÏÇ¨ Ï†ÄÏû• (Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏Ïùò ÏùºÎ∂Ä)
                narrative = response_text[:500] if len(response_text) > 500 else response_text
                
                if not stock.raw_data:
                    stock.raw_data = {}
                stock.raw_data['news_narrative'] = narrative
                stock.raw_data['news_updated_at'] = datetime.now().isoformat()
                session.commit()

                result = {
                    "ticker": ticker,
                    "stock_name": stock.name,
                    "news_count": len(news_list),
                    "sentiment": sentiment,
                    "confidence": 0.75,  # Í∏∞Î≥∏Í∞í
                    "summary": response_text,  # Ï†ÑÏ≤¥ ÎßàÌÅ¨Îã§Ïö¥ ÌÖçÏä§Ìä∏
                    "analyzed_at": datetime.now().isoformat()
                }

                logger.info(
                    f"[NewsAgent] {ticker} Î∂ÑÏÑù ÏôÑÎ£å: {sentiment}"
                )

                return result

            except Exception as e:
                logger.error(f"[NewsAgent] {ticker} Î∂ÑÏÑù Ïã§Ìå®: {e}")
                return {
                    "ticker": ticker,
                    "stock_name": stock.name,
                    "news_count": len(news_list),
                    "error": str(e),
                }
